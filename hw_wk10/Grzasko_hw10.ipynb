{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Document Classification:\n",
    "## CNN or Fox News\n",
    "### Aaron Grzasko\n",
    "### 11/9/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to build a binary classifier model using a collection of text-based documents. \n",
    "\n",
    "Classification is an example of a supervised learning procedure: data are trained to using using labeled target values.  The quality of the model is then assessed against holdout, or test data.\n",
    "\n",
    "In the scripts below, I build a classifer model to determine whether a given news article was generated from [cnn.com](cnn.com) or [foxnews.com](foxnews.com). My hypothesis is that there are material differences in the content of each news site (e.g. word choices, article length, political leanings, etc.) that can be used to accurate determine the source of a particular article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import python modules relevant to the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant modules\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing,naive_bayes, metrics\n",
    "from sklearn import ensemble\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Data Initial Pull**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the *newspaper* python library to scrape news articles from both CNN and Fox News.\n",
    "\n",
    "For the CNN data pull, I limited the categories to opinion and politics-related articles. \n",
    "\n",
    "Below are scripts to generate a list of relevant article urls: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial list of cnn article urls\n",
    "cnn_paper = newspaper.build('http://cnn.com',memoize_articles = False)\n",
    "\n",
    "# pull urls related to opinions and politics on cnn\n",
    "cnn_po_articles = []\n",
    "for article in cnn_paper.articles:\n",
    "    match = re.search( r'/opinion/|/politics/',article.url)\n",
    "    if match and article.url not in cnn_po_articles:\n",
    "        cnn_po_articles.append(article.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cnn urls found\n",
    "len(cnn_po_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *newspaper* library limits the data pull to mostly recent articles.  Because of this limitation, our sample list of urls is small. \n",
    "\n",
    "Let's save the initial url list to a csv file in our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cnn urls to csv file\n",
    "df = pd.DataFrame(cnn_po_articles, columns=[\"cnn_urls\"])\n",
    "df.to_csv('cnn_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download all articles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull article text for each url; save to list\n",
    "cnn_articles = []\n",
    "for art in cnn_po_articles:\n",
    "    #print(art)\n",
    "    cnn_article = Article(url = art)\n",
    "    cnn_article.download()\n",
    "    cnn_article.parse()\n",
    "    cnn_articles.append(cnn_article.text)\n",
    "    time.sleep(15)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will save the articles in csv file in my working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cnn article texts to csv file\n",
    "df = pd.DataFrame(cnn_articles, columns=[\"cnn_articles\"])\n",
    "df.to_csv('cnn_articles.csv', index=False, encoding='utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsequent CNN Data Pulls**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, the initial sample of CNN articles was fairly small.  Fortunately, I was able to pull new additional articles as they were published on the cnn website.\n",
    "\n",
    "The scripts below are slightly modified versions of thes scripts used to do the initial data pull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the original urls and retrieved articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in initial cnn urls\n",
    "cnn_po_articles = pd.read_csv('cnn_urls.csv')\n",
    "\n",
    "# read back in initial cnn articles\n",
    "cnn_articles = pd.read_csv('cnn_articles.csv', encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find new urls, append to the existing url list, and save to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update with new article urls as they become available\n",
    "cnn_paper_new = newspaper.build('http://cnn.com',memoize_articles = False)\n",
    "\n",
    "# generate new article urls\n",
    "cnn_po_articles_new = []\n",
    "for article in cnn_paper_new.articles:\n",
    "    match = re.search( r'/opinion/|/politics/',article.url)\n",
    "    if match and article.url not in list(cnn_po_articles.iloc[:,0]):\n",
    "        cnn_po_articles_new.append(article.url)\n",
    "        \n",
    "# add new articles to existing article urls\n",
    "cnn_po_articles_new = pd.DataFrame(cnn_po_articles_new,columns=[\"cnn_urls\"])\n",
    "cnn_po_articles = pd.concat([cnn_po_articles, cnn_po_articles_new])\n",
    "\n",
    "# write cnn urls with new data to csv file\n",
    "df = pd.DataFrame(cnn_po_articles, columns=[\"cnn_urls\"])\n",
    "df.to_csv('cnn_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to download the new articles.  Once the new articles are downloaded, I can append them to the master file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull article text for each new cnn url; save to list\n",
    "cnn_articles_new = []\n",
    "for art in cnn_po_articles_new.iloc[:,0]:\n",
    "    #print(art)\n",
    "    cnn_article = Article(url = art)\n",
    "    cnn_article.download()\n",
    "    cnn_article.parse()\n",
    "    cnn_articles_new.append(cnn_article.text)\n",
    "    time.sleep(15)\n",
    "    \n",
    "# append new articles to existing list\n",
    "cnn_articles_new = pd.DataFrame(cnn_articles_new,columns=[\"cnn_articles\"])\n",
    "cnn_articles = pd.concat([cnn_articles, cnn_articles_new])\n",
    "\n",
    "# write all cnn article texts to new csv file\n",
    "df = pd.DataFrame(cnn_articles, columns=[\"cnn_articles\"])\n",
    "df.to_csv('cnn_articles.csv', index=False, encoding='utf-16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in cnn articles\n",
    "cnn_articles = pd.read_csv('cnn_articles.csv', encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnn_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 175 total cnn articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fox News Initial Data Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts used to pull Fox News articles are similar to those used for downloading cnn articles.\n",
    "\n",
    "I limited my Fox News article search to those labeled as \"opinion\", \"politics\", or \"insider\".\n",
    "\n",
    "Below are scripts to identify relevant fox urls.  The urls are then saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial list of fox article urls\n",
    "fox_paper = newspaper.build('http://foxnews.com', memoize_articles = False)\n",
    "\n",
    "# newspaper library pulls some duplicate fox articles, some with http, others with https\n",
    "# regex below identifies url after http(s) prefix.\n",
    "re_pat = re.compile(r'(?<=https://).+|(?<=http://).+')\n",
    "\n",
    "# pull urls related to opinions, politics, insidere on fox \n",
    "# make sure to avoid duplicate entries\n",
    "fox_po_articles = []\n",
    "for article in fox_paper.articles:\n",
    "    match = re.search( r'/opinion/|/politics/|/insider',article.url)\n",
    "    if match and re_pat.search(article.url).group(0) not in \" \".join(fox_po_articles):\n",
    "        fox_po_articles.append(article.url)\n",
    "        \n",
    "# write fox urls to csv file\n",
    "df = pd.DataFrame(fox_po_articles, columns=[\"fox_urls\"])\n",
    "df.to_csv('fox_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fox urls identified, I can download the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_articles = []\n",
    "for art in fox_po_articles:\n",
    "    #print(art)\n",
    "    fox_article = Article(url = art)\n",
    "    fox_article.download()\n",
    "    fox_article.parse()\n",
    "    fox_articles.append(fox_article.text)\n",
    "    time.sleep(15)\n",
    "\n",
    "# write fox articles to csv file\n",
    "df = pd.DataFrame(fox_articles, columns=[\"fox_articles\"])\n",
    "df.to_csv('fox_articles.csv', index=False, encoding='utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsequent Fox Data Pulls**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the initial sample of news articles is quite small.  I used the scripts below to augment the initial sample with newer Fox articles.\n",
    "\n",
    "I will first read in the original urls and fox articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in original fox urls\n",
    "fox_po_articles = pd.read_csv('fox_urls.csv')\n",
    "\n",
    "# read back in original fox articles\n",
    "fox_articles = pd.read_csv('fox_articles.csv', encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look for new urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update with new article urls as they become available\n",
    "fox_paper_new = newspaper.build('http://foxnews.com',memoize_articles = False)\n",
    "\n",
    "re_pat = re.compile(r'(?<=https://).+|(?<=http://).+')\n",
    "\n",
    "# pull urls related to opinions and politics on fox\n",
    "fox_po_articles_new = []\n",
    "for article in fox_paper_new.articles:\n",
    "    match = re.search( r'/opinion/|/politics/|/insider',article.url)\n",
    "    if match and re_pat.search(article.url).group(0) not in \" \".join(fox_po_articles.iloc[:,0]) \\\n",
    "    and re_pat.search(article.url).group(0) not in \" \".join(fox_po_articles_new):\n",
    "        fox_po_articles_new.append(article.url)\n",
    "\n",
    "# add new articles to existing article urls\n",
    "fox_po_articles_new = pd.DataFrame(fox_po_articles_new,columns=[\"fox_urls\"])\n",
    "fox_po_articles = pd.concat([fox_po_articles, fox_po_articles_new])\n",
    "\n",
    "# write cnn urls with new data to csv file\n",
    "df = pd.DataFrame(fox_po_articles, columns=[\"fox_urls\"])\n",
    "df.to_csv('fox_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the new articles, and append to the initial file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull article text for each new cnn url; save to list\n",
    "fox_articles_new = []\n",
    "for art in fox_po_articles_new.iloc[:,0]:\n",
    "    #print(art)\n",
    "    fox_article = Article(url = art)\n",
    "    fox_article.download()\n",
    "    fox_article.parse()\n",
    "    fox_articles_new.append(fox_article.text)\n",
    "    time.sleep(15)\n",
    "\n",
    "fox_articles_new = pd.DataFrame(fox_articles_new,columns=[\"fox_articles\"])\n",
    "fox_articles = pd.concat([fox_articles, fox_articles_new])\n",
    "\n",
    "# write fox article texts to csv file\n",
    "df = pd.DataFrame(fox_articles, columns=[\"fox_articles\"])\n",
    "df.to_csv('fox_articles.csv', index=False, encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in fox articles\n",
    "fox_articles = pd.read_csv('fox_articles.csv', encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fox_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have almost 100 Fox News articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Scrubbing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I can build any models, I need to prep the data.\n",
    "\n",
    "Let's begin by removing any references to CNN or Fox News from the articles' text.  We want to determine the origin of each article from contextual clues, not explicit references to the article's source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove source references in cnn articles\n",
    "for i in range(len(cnn_articles)):\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"(CNN)\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"CNN\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"cnn\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"FoxNews\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"Foxnews\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"FOXNEWS\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"foxnews\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"FOX\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"Fox\", \"Outlet\")\n",
    "    cnn_articles.iloc[i,0] = str(cnn_articles.iloc[i,0]).replace(\"fox\", \"Outlet\")\n",
    "    \n",
    "# remove source references in fox articles\n",
    "for i in range(len(fox_articles)):\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"FoxNews\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"Foxnews\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"FOXNEWS\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"foxnews\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"FOX\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"Fox\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"fox\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"(CNN)\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"CNN\", \"Outlet\")\n",
    "    fox_articles.iloc[i,0] = str(fox_articles.iloc[i,0]).replace(\"cnn\", \"Outlet\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create appropriate news source labels for each collection of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn labels\n",
    "cnn_articles['label'] = pd.Series(['cnn']*len(cnn_articles))\n",
    "\n",
    "# fox lables\n",
    "fox_articles['label'] = pd.Series(['fox']*len(fox_articles))\n",
    "\n",
    "# rename article columns to \"text\"\n",
    "cnn_articles = cnn_articles.rename(index=str, columns={\"cnn_articles\": \"text\"})\n",
    "fox_articles = fox_articles.rename(index=str, columns={\"fox_articles\": \"text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the appropriate labels assigned, I can combine the two article dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined dataset with labels\n",
    "combined = pd.concat([cnn_articles,fox_articles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I apply a variety of text tranformations including:\n",
    "- converting text to lowercase\n",
    "- removing punctuation\n",
    "- removing newlines and carriage returns\n",
    "- word stemming using the Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lower case\n",
    "combined['text'] = combined['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove punctuation\n",
    "combined['text'] = combined['text'].apply(lambda s: ''.join(ch for ch in s if ch not in set(string.punctuation)))\n",
    "\n",
    "# remove additional items such as new newlines\n",
    "combined['text'] = combined['text'].apply(lambda s: s.replace('\\n',' '))\n",
    "combined['text'] = combined['text'].apply(lambda s: s.replace('\\r',''))\n",
    "combined['text'] = combined['text'].apply(lambda s: s.replace('  ',' '))\n",
    "\n",
    "# tokenize words\n",
    "combined['text'] = combined['text'].apply(nltk.word_tokenize)  \n",
    "\n",
    "# stem words\n",
    "stemmer = PorterStemmer()\n",
    "combined['text'] = combined['text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "# convert text back to string from list of words\n",
    "combined['text'] = combined['text'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a built-in sklearn function, I will split the combined dataset into training and test components.  I will use 70% of the data for training, and the other 30% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test datasets \n",
    "train_x, test_x, train_y, test_y = train_test_split(combined['text'], combined['label'], random_state=4, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python model require that the target variables be coded with numerical values rather than text.  Below, I convert \"cnn\" and \"fox\" values to 0s and 1s, respectivelys.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target values to numbers\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use tf-idf scores as a possible set of features for the models.  These scores are used to assess the relative importance of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=500)\n",
    "tfidf_vect.fit(combined['text'])\n",
    "train_x_tfidf =  tfidf_vect.transform(train_x)\n",
    "test_x_tfidf =  tfidf_vect.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also produce a set of tf-idf scores for word bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram tfidf\n",
    "tfidf_bi = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=500)\n",
    "tfidf_bi.fit(combined['text'])\n",
    "train_x_tfidf_bi =  tfidf_bi.transform(train_x)\n",
    "test_x_tfidf_bi =  tfidf_bi.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data processed, we can fit a variety of classifer models.  For this assignement, we will four models in total:\n",
    "* Naive Bayes Model using TFIDF features\n",
    "* Naive Bayes Model using TFIDF word bigram features\n",
    "* Random Forest Model using TFIDF features\n",
    "* Tandom Forest Model using TFIDF word bigram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Using TFIDF Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit naive bayes using tf-idf\n",
    "NB_tfidf = MultinomialNB().fit(train_x_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with tfidf, accuracy:  0.6707317073170732\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy on test set\n",
    "predictions = NB_tfidf.predict(test_x_tfidf)\n",
    "    \n",
    "print(\"Naive Bayes with tfidf, accuracy: \",metrics.accuracy_score(predictions, test_y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for NB with TFIDF:\n",
      "[[49  0]\n",
      " [27  6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for NB with TFIDF:\")\n",
    "print(metrics.confusion_matrix(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics for NB with TFIDF:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        cnn       0.64      1.00      0.78        49\n",
      "        fox       1.00      0.18      0.31        33\n",
      "\n",
      "avg / total       0.79      0.67      0.59        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Metrics for NB with TFIDF:\")\n",
    "print(metrics.classification_report(test_y, predictions, target_names = ['cnn','fox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial Naive Bayes model does not great accuracy.  The model also correctly labels only 18% of the total Fox News articles.  On the the other hand, the model has perfect precision for Fox News articles.  In other words, the articles that the model labels as Fox News articles are all correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Using TFIDF Bigram Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit naive bayes using tf-idf bigrams\n",
    "NB_tfidf_bi = MultinomialNB().fit(train_x_tfidf_bi, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with tfidf bigrams, accuracy:  0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy on test set\n",
    "predictions = NB_tfidf_bi.predict(test_x_tfidf_bi)\n",
    "    \n",
    "print(\"Naive Bayes with tfidf bigrams, accuracy: \",metrics.accuracy_score(predictions, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for NB with TFIDF bigrams:\n",
      "[[47  2]\n",
      " [16 17]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for NB with TFIDF bigrams:\")\n",
    "print(metrics.confusion_matrix(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics for NB with TFIDF bigrams:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        cnn       0.75      0.96      0.84        49\n",
      "        fox       0.89      0.52      0.65        33\n",
      "\n",
      "avg / total       0.81      0.78      0.76        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Metrics for NB with TFIDF bigrams:\")\n",
    "print(metrics.classification_report(test_y, predictions, target_names = ['cnn','fox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes model shows significant improvement to accuracy using the TFIDF bigram features.  Accuracy is now roughly 80%.  The recall for Fox News articles is still not great (just over 50%), but recall for cnn is great, and the precision scores for both news sources is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest with TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest tfidf \n",
    "RF_tfidf = ensemble.RandomForestClassifier().fit(train_x_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with tfidf, accuracy:  0.7682926829268293\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy on test set\n",
    "predictions = RF_tfidf.predict(test_x_tfidf)\n",
    "    \n",
    "print(\"Random Forest with tfidf, accuracy: \",metrics.accuracy_score(predictions, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for RF with TFIDF:\n",
      "[[48  1]\n",
      " [18 15]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for RF with TFIDF:\")\n",
    "print(metrics.confusion_matrix(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics for RF with TFIDF:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        cnn       0.73      0.98      0.83        49\n",
      "        fox       0.94      0.45      0.61        33\n",
      "\n",
      "avg / total       0.81      0.77      0.75        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Metrics for RF with TFIDF:\")\n",
    "print(metrics.classification_report(test_y, predictions, target_names = ['cnn','fox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model trained on TFIDF scores produces a model that is similar in quality to the Naive Bayes Model using TFIDF bigram data.  The model recall for fox news articles is still not very good (under 50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest with TFIDF Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest tfidf on tfidf bigrams\n",
    "RF_tfidf_bi = ensemble.RandomForestClassifier().fit(train_x_tfidf_bi, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with tfidf bigrams, accuracy:  0.8536585365853658\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy on test set\n",
    "predictions = RF_tfidf_bi.predict(test_x_tfidf_bi)\n",
    "    \n",
    "print(\"Random Forest with tfidf bigrams, accuracy: \",metrics.accuracy_score(predictions, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for RF with TFIDF bigrams:\n",
      "[[49  0]\n",
      " [12 21]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for RF with TFIDF bigrams:\")\n",
    "print(metrics.confusion_matrix(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics for RF with TFIDF bigrams:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        cnn       0.80      1.00      0.89        49\n",
      "        fox       1.00      0.64      0.78        33\n",
      "\n",
      "avg / total       0.88      0.85      0.85        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Metrics for RF with TFIDF bigrams:\")\n",
    "print(metrics.classification_report(test_y, predictions, target_names = ['cnn','fox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model, trained on TFIDF bigrams, produced the most accurate result on the holdout data.  Accuracy is over 85%.  Fox News recall is acceptable, with a value of 64%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://youtu.be/M5v20YSogoM?hd=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- newspaper module:  https://newspaper.readthedocs.io/en/latest/user_guide/quickstart.html#parsing-an-article\n",
    "- classification models: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "- More classification models: https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
